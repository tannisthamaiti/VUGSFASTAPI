{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+hdRjPoy0f44uxWpdAz0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAHIL9581/w2w/blob/main/W2W_WNB_INFERENCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Setup Environment & Install Libraries\n",
        "\n",
        "# --- 1. Install Required Libraries ---\n",
        "print(\"--> Installing libraries for inference...\")\n",
        "!pip install wandb torch lasio scikit-learn pandas matplotlib joblib pyyaml -q\n",
        "print(\"✅ Installation complete.\")\n",
        "\n",
        "# --- 2. Create Temporary Workspace ---\n",
        "import os\n",
        "os.makedirs(\"/content/inference_data\", exist_ok=True)\n",
        "os.makedirs(\"/content/inference_artifacts\", exist_ok=True)\n",
        "os.chdir(\"/content/inference_data\")\n",
        "print(f\"✅ Workspace created at {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9byIPjSIOHr",
        "outputId": "980b0364-836e-4469-85a8-25a003a781d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Installing libraries for inference...\n",
            "✅ Installation complete.\n",
            "✅ Workspace created at /content/inference_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Login to Weights & Biases\n",
        "import wandb\n",
        "\n",
        "print(\"--> ACTION REQUIRED: Please log in to your Weights & Biases account.\")\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxWldlg7IOKC",
        "outputId": "34197905-91f5-4dd5-d08c-421f836c7bc7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> ACTION REQUIRED: Please log in to your Weights & Biases account.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msahilpareek203\u001b[0m (\u001b[33msahilpareek203-amrita-vishwa-vidyapeetham\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Configure Inference Run\n",
        "\n",
        "# --- ACTION REQUIRED: Fill in your W&B project details ---\n",
        "# You can find these details in the URL of your previous W&B runs.\n",
        "# Example URL: https://wandb.ai/your-entity/your-project/runs/...\n",
        "\n",
        "WANDB_ENTITY = \"sahilpareek203-amrita-vishwa-vidyapeetham\"  # Your W&B username or team name\n",
        "WANDB_PROJECT = \"W2W_Matcher_Pipeline_Notebook\"       # The project name where your model was saved\n",
        "MODEL_VERSION = \"latest\"  # Use \"latest\" to get the most recent model, or a specific version like \"v0\"\n",
        "\n",
        "# --- Leave the rest of the config as is ---\n",
        "config = {\n",
        "    \"paths\": {\n",
        "        \"processed_csv_path\": \"full_well_data.csv\",\n",
        "        \"label_encoder_path\": \"inference_artifacts/label_encoder.json\",\n",
        "        \"std_scaler_path\": \"inference_artifacts/StandardScaler.bin\",\n",
        "        \"final_model_path\": \"inference_artifacts/final_model.pt\"\n",
        "    },\n",
        "    \"finetuning\": {\n",
        "        \"model_params\": {\n",
        "            \"patch_height\": 700, \"act_name\": \"prelu\",\n",
        "            \"hidden_dim\": 256, \"num_queries\": 100,\n",
        "            \"num_heads\": 8, \"dropout\": 0.1,\n",
        "            \"num_transformers\": 6, \"output_size\": 3\n",
        "        }\n",
        "    },\n",
        "    \"inference\": {\n",
        "        \"correlation_threshold\": 0.7\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"✅ Inference configuration set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH1jOIAyIOMT",
        "outputId": "c18cbf21-6b48-4216-e8e8-733955f6d025"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference configuration set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Download Production Artifacts from W&B (Final Corrected Version)\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "print(\"--> Connecting to W&B to download production artifacts...\")\n",
        "# Initialize the API to access the project\n",
        "api = wandb.Api()\n",
        "entity_project_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}\"\n",
        "\n",
        "try:\n",
        "    # Download the main trained model\n",
        "    model_artifact_name = f'boundary-detector-model:{MODEL_VERSION}'\n",
        "    print(f\"--> Downloading model: {model_artifact_name}\")\n",
        "    model_artifact = api.artifact(f'{entity_project_path}/{model_artifact_name}', type='model')\n",
        "    model_artifact.download(root=os.path.dirname(config['paths']['final_model_path']))\n",
        "    print(\"✅ Model downloaded successfully.\")\n",
        "\n",
        "    # Download the StandardScaler\n",
        "    scaler_artifact_name = 'StandardScaler:latest'\n",
        "    print(f\"--> Downloading StandardScaler: {scaler_artifact_name}\")\n",
        "    scaler_artifact = api.artifact(f'{entity_project_path}/{scaler_artifact_name}', type='preprocessor')\n",
        "    scaler_artifact.download(root=os.path.dirname(config['paths']['std_scaler_path']))\n",
        "    print(\"✅ StandardScaler downloaded successfully.\")\n",
        "\n",
        "    # Download the Label Encoder\n",
        "    encoder_artifact_name = 'LabelEncoder:latest'\n",
        "    print(f\"--> Downloading Label Encoder: {encoder_artifact_name}\")\n",
        "    encoder_artifact = api.artifact(f'{entity_project_path}/{encoder_artifact_name}', type='preprocessor')\n",
        "    encoder_artifact.download(root=os.path.dirname(config['paths']['label_encoder_path']))\n",
        "    print(\"✅ Label Encoder downloaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🚨 ERROR DOWNLOADING ARTIFACTS 🚨\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"\\nPLEASE CHECK THE FOLLOWING:\")\n",
        "    print(\"1. Did you successfully re-run the TRAINING notebook with the corrected Cell 13?\")\n",
        "    print(f\"2. Do you see 'StandardScaler' and 'LabelEncoder' in the 'Artifacts' tab of your W&B project: https://wandb.ai/{entity_project_path}/artifacts\")\n",
        "    print(f\"3. Are the WANDB_ENTITY and WANDB_PROJECT names in Cell 3 of THIS notebook spelled correctly?\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "print(\"\\n--- All necessary artifacts should now be available locally. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8J4bKVqIOTz",
        "outputId": "8d8bf354-b90d-4e43-cc1a-be01769397c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Connecting to W&B to download production artifacts...\n",
            "--> Downloading model: boundary-detector-model:latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model downloaded successfully.\n",
            "--> Downloading StandardScaler: StandardScaler:latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ StandardScaler downloaded successfully.\n",
            "--> Downloading Label Encoder: LabelEncoder:latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label Encoder downloaded successfully.\n",
            "\n",
            "--- All necessary artifacts should now be available locally. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Upload Full LAS Dataset (ZIP file)\n",
        "from google.colab import files\n",
        "import os\n",
        "import pandas as pd\n",
        "import lasio\n",
        "from joblib import load\n",
        "import json\n",
        "\n",
        "print(\">>> ACTION REQUIRED: Please upload the same ZIP file with all .las files used for training.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n✅ '{zip_filename}' uploaded. Processing into a single CSV for lookup...\")\n",
        "\n",
        "    # Unzip and process the files\n",
        "    os.makedirs(\"raw_las\", exist_ok=True)\n",
        "    !unzip -q -o \"{zip_filename}\" -d raw_las/\n",
        "\n",
        "    all_wells_df, las_files_found = [], []\n",
        "    for root, dirs, files in os.walk(\"raw_las\"):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.las'):\n",
        "                las_files_found.append(os.path.join(root, file))\n",
        "\n",
        "    for filepath in las_files_found:\n",
        "        try:\n",
        "            las = lasio.read(filepath)\n",
        "            df = las.df().reset_index()\n",
        "            df['WELL'] = las.well.WELL.value or os.path.splitext(os.path.basename(filepath))[0]\n",
        "            df['GROUP'] = 'UNKNOWN'\n",
        "            for param in las.params:\n",
        "                if 'GROUP' in param.mnemonic.upper(): df['GROUP'] = param.value\n",
        "            all_wells_df.append(df)\n",
        "        except Exception as e: print(f\"    - Could not read {filepath}: {e}\")\n",
        "\n",
        "    master_df = pd.concat(all_wells_df, ignore_index=True)\n",
        "    if 'DEPT' in master_df.columns: master_df.rename(columns={'DEPT': 'DEPTH_MD'}, inplace=True)\n",
        "    master_df.to_csv(config['paths']['processed_csv_path'], index=False, sep=';')\n",
        "\n",
        "    print(f\"\\n✅ Successfully processed {len(las_files_found)} files into '{config['paths']['processed_csv_path']}'.\")\n",
        "\n",
        "    # Print available wells for convenience\n",
        "    print(\"\\n--- Available Well Names for Correlation ---\")\n",
        "    for well in sorted(master_df['WELL'].unique()):\n",
        "        print(f\"- {well}\")\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    # Clean up\n",
        "    os.remove(zip_filename)\n",
        "    !rm -rf raw_las\n",
        "else:\n",
        "    print(\"\\n⚠️ Upload cancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19a6vVSiIOWi",
        "outputId": "d63ab56f-0a96-440d-8b12-9f547a4ab050"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> ACTION REQUIRED: Please upload the same ZIP file with all .las files used for training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e3c570c9-5c0f-436d-a66f-7ad092607c8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e3c570c9-5c0f-436d-a66f-7ad092607c8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.zip to train.zip\n",
            "\n",
            "✅ 'train.zip' uploaded. Processing into a single CSV for lookup...\n",
            "\n",
            "✅ Successfully processed 118 files into 'full_well_data.csv'.\n",
            "\n",
            "--- Available Well Names for Correlation ---\n",
            "- 15/9-13 Sleipner East Appr\n",
            "- 15/9-14\n",
            "- 15/9-15 Gungne\n",
            "- 15/9-17\n",
            "- 15/9-23 Skardkollen\n",
            "- 16/1-2  Ivar Aasen Appr\n",
            "- 16/1-6 A Verdandi Appr\n",
            "- 16/10-1 Alpha\n",
            "- 16/10-2 Delta\n",
            "- 16/10-3 Tyr Central\n",
            "- 16/10-5 Isbjoern\n",
            "- 16/11-1S T3\n",
            "- 16/2-11 A Johan Sverdrup Appr\n",
            "- 16/2-16 Johan Sverdrup Appr\n",
            "- 16/2-6 Johan Sverdrup\n",
            "- 16/2-7 Johan Sverdrup Appr\n",
            "- 16/4-1\n",
            "- 16/5-3 Johan Sverdrup Appr\n",
            "- 16/7-4 Sigyn\n",
            "- 16/7-5\n",
            "- 16/7-6\n",
            "- 16/8-1\n",
            "- 17/11-1\n",
            "- 17/4-1\n",
            "- 25/10-10  Balder Triassic\n",
            "- 25/10-9 Aegis\n",
            "- 25/11-15  Grane\n",
            "- 25/11-19 S  Balder Appr\n",
            "- 25/11-24 Jakob South\n",
            "- 25/11-5 Balder Appr\n",
            "- 25/2-13 T4\n",
            "- 25/2-14 Froey Appr\n",
            "- 25/2-7\n",
            "- 25/3-1\n",
            "- 25/4-5\n",
            "- 25/5-1 Froey\n",
            "- 25/5-3  Skirne\n",
            "- 25/5-4  Byggve\n",
            "- 25/6-1\n",
            "- 25/6-2  Delta-Beta\n",
            "- 25/6-3\n",
            "- 25/7-2\n",
            "- 25/8-5 S  Jotun\n",
            "- 25/8-7  Krap 1\n",
            "- 25/9-1  Rummel\n",
            "- 26/4-1\n",
            "- 29/3-1\n",
            "- 29/6-1\n",
            "- 30/3-3\n",
            "- 30/3-5S\n",
            "- 30/6-5\n",
            "- 31/2-1\n",
            "- 31/2-10\n",
            "- 31/2-19 S\n",
            "- 31/2-21 S\n",
            "- 31/2-7\n",
            "- 31/2-8\n",
            "- 31/2-9\n",
            "- 31/3-1\n",
            "- 31/3-2\n",
            "- 31/3-3\n",
            "- 31/3-4\n",
            "- 31/4-10\n",
            "- 31/4-5\n",
            "- 31/5-4 S\n",
            "- 31/6-5\n",
            "- 31/6-8\n",
            "- 32/2-1\n",
            "- 33/5-2\n",
            "- 33/6-3 S\n",
            "- 33/9-1\n",
            "- 33/9-17\n",
            "- 34/10-16R\n",
            "- 34/10-19\n",
            "- 34/10-21\n",
            "- 34/10-33\n",
            "- 34/10-35\n",
            "- 34/11-1\n",
            "- 34/11-2 S\n",
            "- 34/12-1\n",
            "- 34/2-4\n",
            "- 34/3-1 A\n",
            "- 34/3-2 S\n",
            "- 34/3-3 A\n",
            "- 34/4-10 R\n",
            "- 34/5-1 A\n",
            "- 34/5-1 S\n",
            "- 34/6-1\n",
            "- 34/7-13\n",
            "- 34/7-20\n",
            "- 34/7-21\n",
            "- 34/8-1\n",
            "- 34/8-3\n",
            "- 34/8-7R\n",
            "- 35/11-1\n",
            "- 35/11-10\n",
            "- 35/11-11\n",
            "- 35/11-12\n",
            "- 35/11-13\n",
            "- 35/11-15 S\n",
            "- 35/11-5\n",
            "- 35/11-6\n",
            "- 35/11-7\n",
            "- 35/12-1\n",
            "- 35/3-7 S\n",
            "- 35/4-1\n",
            "- 35/6-2 S\n",
            "- 35/8-4\n",
            "- 35/8-6 S\n",
            "- 35/9-10 S\n",
            "- 35/9-2\n",
            "- 35/9-5\n",
            "- 35/9-6 S\n",
            "- 35/9-7\n",
            "- 35/9-8\n",
            "- 36/7-3\n",
            "- 7/1-1\n",
            "- 7/1-2 S\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Define Model Architectures\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_activation(name): return nn.PReLU() if name == 'prelu' else nn.ReLU() if name == 'relu' else nn.GELU()\n",
        "\n",
        "class Block1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2, kernel_size=3, activation='prelu'):\n",
        "        super().__init__(); self.b = nn.Sequential(nn.Conv1d(in_channels,out_channels,kernel_size,stride,padding=kernel_size//2), nn.BatchNorm1d(out_channels), get_activation(activation), nn.Conv1d(out_channels,out_channels,kernel_size,1,padding=kernel_size//2), nn.BatchNorm1d(out_channels), get_activation(activation))\n",
        "    def forward(self, x): return self.b(x)\n",
        "\n",
        "class UNetEncoder1D(nn.Module):\n",
        "    def __init__(self, in_channels, activation='prelu'):\n",
        "        super().__init__(); self.start=Block1D(in_channels,32,stride=1,activation=activation); self.e1=Block1D(32,64,stride=2,activation=activation); self.e2=Block1D(64,128,stride=2,activation=activation); self.e3=Block1D(128,256,stride=2,activation=activation); self.mid=Block1D(256,512,stride=2,activation=activation)\n",
        "    def forward(self, x): x=x.squeeze(1).permute(0,2,1); s1=self.start(x); s2=self.e1(s1); s3=self.e2(s2); s4=self.e3(s3); return self.mid(s4)\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self,i,o): super().__init__(); self.l=nn.Linear(i,o)\n",
        "    def forward(self,x): return self.l(x.flatten(1))\n",
        "\n",
        "class Query(nn.Module):\n",
        "    def __init__(self,s,d): super().__init__(); self.q=nn.Parameter(torch.randn(1,s,d))\n",
        "    def forward(self,x): return self.q.repeat(x.shape[0],1,1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,i,n,d): super().__init__(); self.t=nn.TransformerEncoderLayer(d_model=i,nhead=n,dropout=d,batch_first=True,dim_feedforward=i*4)\n",
        "    def forward(self,q,c): return self.t(q)\n",
        "\n",
        "class W2WTransformerModel(nn.Module):\n",
        "    def __init__(self,c):\n",
        "        super().__init__()\n",
        "        p=c['finetuning']['model_params']\n",
        "        # Dynamically calculate the flattened feature size\n",
        "        with torch.no_grad():\n",
        "            dummy_encoder = UNetEncoder1D(p['in_channels'], p['act_name'])\n",
        "            dummy_output = dummy_encoder(torch.randn(1, 1, p['patch_height'], p['in_channels']))\n",
        "            p_in = dummy_output.flatten(1).shape[1]\n",
        "\n",
        "        self.encoder = UNetEncoder1D(p['in_channels'], p['act_name'])\n",
        "        self.project = Project(p_in, p['hidden_dim'])\n",
        "        self.query = Query(p['num_queries'], p['hidden_dim'])\n",
        "        self.transformers = nn.ModuleList([Transformer(p['hidden_dim'], p['num_heads'], p['dropout']) for _ in range(p['num_transformers'])])\n",
        "        self.finalize = nn.Sequential(nn.Linear(p['hidden_dim'], p['output_size']), get_activation(p['act_name']), nn.LayerNorm(p['output_size']))\n",
        "\n",
        "    def forward(self,img):\n",
        "        seq = self.project(self.encoder(img)).unsqueeze(1)\n",
        "        q = self.query(seq)\n",
        "        for t in self.transformers: q = t(q, seq)\n",
        "        return self.finalize(q)\n",
        "\n",
        "print(\"✅ Model architectures defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXErWylDJbAL",
        "outputId": "0491a2f8-6745-4494-8f83-fab1ef768d7b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model architectures defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. Define Plotting Logic\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "def plot_well_correlation(well1,well2,layers1,layers2,matrix,threshold,path):\n",
        "    plt.style.use('ggplot'); fig,ax=plt.subplots(figsize=(10,12))\n",
        "    if not layers1 or not layers2: print(f'Warning: Plotting skipped. Well 1 layers:{len(layers1)}, Well 2:{len(layers2)}'); return\n",
        "    max_depth=max(layers1[-1]['bottom'],layers2[-1]['bottom']) if layers1 and layers2 else 1000\n",
        "    ax.set_ylim(max_depth+50,-50); ax.set_xlim(-0.5,2.5)\n",
        "    n1=len(set(l['Group'] for l in layers1)); n2=len(set(l['Group'] for l in layers2))\n",
        "    for l in layers1: ax.add_patch(patches.Rectangle((0,l['Top']),1,l['Height'],ec='k',fc=plt.cm.viridis(l['Group']/(n1 if n1>0 else 1)),alpha=0.6))\n",
        "    for l in layers2: ax.add_patch(patches.Rectangle((1.5,l['Top']),1,l['Height'],ec='k',fc=plt.cm.viridis(l['Group']/(n2 if n2>0 else 1)),alpha=0.6))\n",
        "    for i,row in enumerate(matrix):\n",
        "        for j,sim in enumerate(row):\n",
        "            if sim>=threshold: ax.add_patch(patches.Polygon([[1,layers1[i]['Top']],[1,layers1[i]['bottom']],[1.5,layers2[j]['bottom']],[1.5,layers2[j]['Top']]],fc=plt.cm.Greens(sim),alpha=0.5))\n",
        "    ax.set_xticks([0.5,2]); ax.set_xticklabels([well1,well2],fontsize=14); ax.set_ylabel('Depth',fontsize=12); ax.set_title('Well to Well Correlation',fontsize=16); plt.savefig(path); plt.close()\n",
        "    print(f'--> Correlation plot saved to {path}')\n",
        "\n",
        "def generate_single_correlation_plot(config,full_data,ref_name,woi_name,out_path):\n",
        "    inf,p=config['inference'],config['paths']; ref_df,woi_df=full_data[full_data['WELL']==ref_name],full_data[full_data['WELL']==woi_name]\n",
        "    if ref_df.empty or woi_df.empty: print(f\"Error: Could not find '{ref_name}' or '{woi_name}'. Please check names.\"); return False\n",
        "    with open(p['label_encoder_path']) as f: le=json.load(f)\n",
        "    def get_layers(df):\n",
        "        df=df.copy().reset_index(drop=True); df['gid']=df['GROUP'].astype(str).map(le).fillna(-1).astype(int); b=np.where(df['gid'].iloc[:-1].values!=df['gid'].iloc[1:].values)[0]+1\n",
        "        indices=np.concatenate(([0],b,[len(df)])); layers=[]\n",
        "        for i in range(len(indices)-1):\n",
        "            s,e=indices[i],indices[i+1]\n",
        "            if s<e: layers.append({'Top':df['DEPTH_MD'].iloc[s],'bottom':df['DEPTH_MD'].iloc[e-1],'Height':df['DEPTH_MD'].iloc[e-1]-df['DEPTH_MD'].iloc[s],'Group':df['gid'].iloc[s]})\n",
        "        return layers\n",
        "    ref_l,woi_l=get_layers(ref_df),get_layers(woi_df); sim=np.zeros((len(ref_l),len(woi_l)))\n",
        "\n",
        "    # This is the MOCK INFERENCE part. A real implementation would use the model here.\n",
        "    print('--> MOCK INFERENCE: Using ground truth layers for visualization.')\n",
        "    for i,l1 in enumerate(ref_l):\n",
        "        for j,l2 in enumerate(woi_l): sim[i,j]=np.random.uniform(0.8,0.95) if l1['Group']==l2['Group'] and l1['Group']!=-1 else np.random.uniform(0.1,0.4)\n",
        "\n",
        "    plot_well_correlation(ref_name,woi_name,ref_l,woi_l,sim,inf['correlation_threshold'],out_path); return True\n",
        "\n",
        "print(\"✅ Plotting logic defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPy02Cp_JbDu",
        "outputId": "77c2b931-cf79-4238-96e1-c6dc1862661b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Plotting logic defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8. 🚀 Inference Dashboard: Generate and Log Plots\n",
        "\n",
        "import torch\n",
        "from joblib import load\n",
        "import pandas as pd\n",
        "\n",
        "# --- ACTION REQUIRED: Define the well pairs you want to plot ---\n",
        "# Copy and paste valid well names from the output of Cell 5.\n",
        "well_pairs_to_plot = [\n",
        "    (\"15_9-13 Sleipner East Appr\", \"16/1-2  Ivar Aasen Appr\"),\n",
        "    (\"16/2-6 Johan Sverdrup\", \"16/5-3 Johan Sverdrup Appr\"),\n",
        "    (\"35/11-1\", \"35/11-6\"),\n",
        "    # Add more pairs here...\n",
        "]\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "print(\"--- Initializing Inference Run ---\")\n",
        "# 1. Dynamically get the number of input features from the saved scaler\n",
        "scaler = load(config['paths']['std_scaler_path'])\n",
        "config['finetuning']['model_params']['in_channels'] = scaler.n_features_in_\n",
        "print(f\"Loaded scaler with {scaler.n_features_in_} features.\")\n",
        "\n",
        "# 2. Load the trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = W2WTransformerModel(config).to(device)\n",
        "model.load_state_dict(torch.load(config['paths']['final_model_path'], map_location=device))\n",
        "model.eval()\n",
        "print(f\"✅ Model '{config['paths']['final_model_path']}' loaded successfully onto {device}.\")\n",
        "\n",
        "# 3. Load the full dataset for lookups\n",
        "full_data = pd.read_csv(config['paths']['processed_csv_path'], delimiter=';')\n",
        "print(f\"✅ Full dataset with {len(full_data.WELL.unique())} wells loaded.\")\n",
        "\n",
        "# 4. Generate plots and log to W&B\n",
        "with wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, job_type='inference-dashboard') as run:\n",
        "    print(f\"\\n--> W&B Run for multiple plots started. View at: {run.url}\")\n",
        "    plots_to_log = {}\n",
        "    for i, (well1, well2) in enumerate(well_pairs_to_plot):\n",
        "        print(f\"\\n--- Generating plot for: {well1} vs {well2} ---\")\n",
        "        # Create a filesystem-safe filename\n",
        "        safe_well1 = well1.replace('/','-').replace(' ','_')\n",
        "        safe_well2 = well2.replace('/','-').replace(' ','_')\n",
        "        output_filename = f\"correlation_{safe_well1}_vs_{safe_well2}.png\"\n",
        "\n",
        "        # NOTE: This still uses the MOCK inference logic.\n",
        "        # To use the real model, you would pass data patches through `model(patches)`\n",
        "        # and interpret the output to create the similarity matrix.\n",
        "        success = generate_single_correlation_plot(config, full_data, well1, well2, output_filename)\n",
        "\n",
        "        if success:\n",
        "            plots_to_log[f\"Plot_{i+1}_{well1}_vs_{well2}\"] = wandb.Image(output_filename)\n",
        "\n",
        "    if plots_to_log:\n",
        "        print(\"\\n--> Logging all plots to Weights & Biases...\")\n",
        "        wandb.log(plots_to_log)\n",
        "        print(\"✅ All plots logged successfully.\")\n",
        "    else:\n",
        "        print(\"\\n--> No plots were generated to log.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "k7SBxcLAJbF1",
        "outputId": "8128cc14-c3cb-4def-bcfb-856c259da00f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Inference Run ---\n",
            "Loaded scaler with 25 features.\n",
            "✅ Model 'inference_artifacts/final_model.pt' loaded successfully onto cpu.\n",
            "✅ Full dataset with 118 wells loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/inference_data/wandb/run-20250720_150848-hql4nox7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7' target=\"_blank\">prime-pond-93</a></strong> to <a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook' target=\"_blank\">https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7' target=\"_blank\">https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> W&B Run for multiple plots started. View at: https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7\n",
            "\n",
            "--- Generating plot for: 15_9-13 Sleipner East Appr vs 16/1-2  Ivar Aasen Appr ---\n",
            "Error: Could not find '15_9-13 Sleipner East Appr' or '16/1-2  Ivar Aasen Appr'. Please check names.\n",
            "\n",
            "--- Generating plot for: 16/2-6 Johan Sverdrup vs 16/5-3 Johan Sverdrup Appr ---\n",
            "--> MOCK INFERENCE: Using ground truth layers for visualization.\n",
            "--> Correlation plot saved to correlation_16-2-6_Johan_Sverdrup_vs_16-5-3_Johan_Sverdrup_Appr.png\n",
            "\n",
            "--- Generating plot for: 35/11-1 vs 35/11-6 ---\n",
            "--> MOCK INFERENCE: Using ground truth layers for visualization.\n",
            "--> Correlation plot saved to correlation_35-11-1_vs_35-11-6.png\n",
            "\n",
            "--> Logging all plots to Weights & Biases...\n",
            "✅ All plots logged successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">prime-pond-93</strong> at: <a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7' target=\"_blank\">https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook/runs/hql4nox7</a><br> View project at: <a href='https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook' target=\"_blank\">https://wandb.ai/sahilpareek203-amrita-vishwa-vidyapeetham/W2W_Matcher_Pipeline_Notebook</a><br>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250720_150848-hql4nox7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}