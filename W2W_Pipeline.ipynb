{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHS5izslMuPN1yY2jMQgdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAHIL9581/w2w/blob/main/W2W_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- 1. SETUP THE TEMPORARY ENVIRONMENT AND WORKSPACE ---\n",
        "PROJECT_PATH = \"/content/W2W_Pipeline\"\n",
        "print(f\"--> Creating project workspace at: {PROJECT_PATH}\")\n",
        "\n",
        "# Clean up previous runs if they exist\n",
        "if os.path.exists(PROJECT_PATH):\n",
        "    shutil.rmtree(PROJECT_PATH)\n",
        "\n",
        "os.makedirs(f\"{PROJECT_PATH}/data/raw_las_files\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/artifacts\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/autoencoder\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/boundary_detector\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/results\", exist_ok=True) # For saving plots\n",
        "\n",
        "# Change the current working directory to the project path\n",
        "%cd {PROJECT_PATH}\n",
        "print(f\"--> Successfully changed directory to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- 2. INSTALL ALL REQUIRED LIBRARIES ---\n",
        "print(\"\\n--> Installing necessary Python libraries (this may take a few minutes)...\")\n",
        "!pip install -U \"ray[train,tune]>=2.9.0\" mlflow torch torchvision torchaudio lasio scikit-learn pandas tqdm matplotlib joblib pyyaml pyngrok -q\n",
        "print(\"✅ Library installation complete.\")"
      ],
      "metadata": {
        "id": "PyKkJucHuUej",
        "outputId": "b034ad76-613a-40da-e58c-285a62c5eff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Creating project workspace at: /content/W2W_Pipeline\n",
            "/content/W2W_Pipeline\n",
            "--> Successfully changed directory to: /content/W2W_Pipeline\n",
            "\n",
            "--> Installing necessary Python libraries (this may take a few minutes)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m876.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Library installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After running the previous cell, execute this cell and then\n",
        "# GO TO \"Runtime\" -> \"Restart runtime...\" in the Colab menu.\n",
        "# Confirm the restart, and then proceed to the next cell.\n",
        "\n",
        "print(\"!!! IMPORTANT: Please RESTART YOUR COLAB RUNTIME NOW !!!\")\n",
        "print(\"Go to 'Runtime' -> 'Restart runtime...' in the Colab menu.\")\n",
        "print(\"Once restarted, continue to the next cell.\")"
      ],
      "metadata": {
        "id": "lyn7CWmyuUhS",
        "outputId": "320e4b97-bf1a-4fce-adb7-3825f0e5e633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! IMPORTANT: Please RESTART YOUR COLAB RUNTIME NOW !!!\n",
            "Go to 'Runtime' -> 'Restart runtime...' in the Colab menu.\n",
            "Once restarted, continue to the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Configure Your Pipeline Run\n",
        "#@markdown ### 1. Select Which Pipelines to Run\n",
        "run_data_preparation = True #@param {type:\"boolean\"}\n",
        "run_pretraining = True #@param {type:\"boolean\"}\n",
        "run_finetuning = True #@param {type:\"boolean\"}\n",
        "run_inference = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Configure Model and Data Parameters\n",
        "#@markdown **Important:** Set the number of feature columns (curves) from your LAS files.\n",
        "input_channels = 13 #@param {type:\"integer\"}\n",
        "#@markdown Set the patch height (window size) for processing well logs.\n",
        "patch_height = 700 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 3. Configure Training Parameters\n",
        "#@markdown Hyperparameter tuning samples for pre-training.\n",
        "pretrain_num_samples = 10 #@param {type:\"integer\"}\n",
        "#@markdown Number of epochs for final model fine-tuning.\n",
        "finetune_epochs = 100 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 4. Configure Inference Parameters\n",
        "#@markdown Provide the exact WELL names to compare (find in `data/train.csv` after data prep).\n",
        "reference_well = \"WELL_NAME_A\" #@param {type:\"string\"}\n",
        "well_of_interest = \"WELL_NAME_B\" #@param {type:\"string\"}\n",
        "#@markdown Confidence threshold (0 to 1) for a predicted boundary to be considered valid.\n",
        "correlation_threshold = 0.7 #@param {type:\"number\"}\n",
        "\n",
        "# ==============================================================================\n",
        "# All imports needed for the notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lasio\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from joblib import dump, load\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import textwrap\n",
        "import yaml\n",
        "import argparse\n",
        "import shutil\n",
        "import mlflow\n",
        "from ray import tune\n",
        "from ray.train import RunConfig, ScalingConfig, Checkpoint\n",
        "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
        "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
        "from google.colab import files\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==============================================================================\n",
        "# Authenticate ngrok with your API key\n",
        "ngrok.set_auth_token(\"2zsOgKyrXbZYrtq5ojgMgwih7AQ_4QwzvhaUh1PL7MYgYB9nY\")\n",
        "# ==============================================================================\n",
        "\n",
        "# Create the configuration dictionary from the form values\n",
        "config = {\n",
        "    'run_data_preparation': run_data_preparation,\n",
        "    'run_pretraining': run_pretraining,\n",
        "    'run_finetuning': run_finetuning,\n",
        "    'run_inference': run_inference,\n",
        "    'paths': {\n",
        "        'raw_las_folder': \"data/raw_las_files/\",\n",
        "        'processed_csv_path': \"data/train.csv\",\n",
        "        'label_encoder_path': \"artifacts/label_encoder.json\",\n",
        "        'std_scaler_path': \"artifacts/StandardScaler.bin\",\n",
        "        'pretrained_encoder_path': \"trained_models/autoencoder/best_autoencoder.pt\",\n",
        "        'final_model_path': \"trained_models/boundary_detector/final_model.pt\",\n",
        "        'results_path': \"results/\"\n",
        "    },\n",
        "    'mlflow': {\n",
        "        'experiment_name': \"W2W_Matcher_Pipeline\"\n",
        "    },\n",
        "    'pretraining': {\n",
        "        'epochs': 25,\n",
        "        'num_samples': pretrain_num_samples,\n",
        "        'search_space': {\n",
        "            'in_channels': tune.grid_search([input_channels]), # Use the value from the form\n",
        "            'optimizer': tune.choice([\"RMSprop\", \"AdamW\", \"Adam\"]),\n",
        "            'lr': tune.loguniform(1e-4, 1e-3),\n",
        "            'act_name': tune.choice([\"prelu\", \"relu\"]),\n",
        "            'batch_size': tune.choice([16, 32]),\n",
        "        }\n",
        "    },\n",
        "    'finetuning': {\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 16,\n",
        "        'epochs': finetune_epochs,\n",
        "        'model_params': {\n",
        "            'patch_height': patch_height,\n",
        "            'in_channels': input_channels,\n",
        "            'act_name': \"prelu\",\n",
        "            'project_in_features': 2048,\n",
        "            'hidden_dim': 256,\n",
        "            'num_queries': 100,\n",
        "            'num_heads': 8,\n",
        "            'dropout': 0.1,\n",
        "            'expansion_factor': 4,\n",
        "            'num_transformers': 6,\n",
        "            'output_size': 3 # [class_prob, top, height]\n",
        "        },\n",
        "        'matcher_costs': {'set_cost_class': 1, 'set_cost_bbox': 5},\n",
        "        'loss_weights': {'loss_matching': 1.0, 'loss_unmatching': 0.5, 'loss_height_constraint': 0.5}\n",
        "    },\n",
        "    'inference': {\n",
        "        'reference_well': reference_well,\n",
        "        'well_of_interest': well_of_interest,\n",
        "        'correlation_threshold': correlation_threshold,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"✅ Configuration loaded successfully.\")\n",
        "print(f\"Project directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "0sgIFFFtuUji",
        "outputId": "8f0580a3-874c-476c-d78f-84eb4ebf5261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuration loaded successfully.\n",
            "Project directory: /content/W2W_Pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"\\n⚠️ Upload was cancelled or failed. Please run this cell again.\")\n",
        "elif len(uploaded_files) > 1:\n",
        "    print(\"\\n⚠️ Please upload only a single ZIP file. Run this cell again.\")\n",
        "else:\n",
        "    zip_filename = list(uploaded_files.keys())[0]\n",
        "    print(f\"\\n✅ '{zip_filename}' uploaded successfully.\")\n",
        "    print(\"--> Unzipping into the 'data/raw_las_files' folder...\")\n",
        "    # Use -o to overwrite existing files without prompting\n",
        "    !unzip -q -o \"{zip_filename}\" -d data/raw_las_files/\n",
        "    print(\"--> ZIP file has been unzipped successfully.\")\n",
        "    # Clean up the uploaded zip file\n",
        "    os.remove(zip_filename)\n",
        "    print(\"\\n✅ Data input step is complete. You can now proceed.\")"
      ],
      "metadata": {
        "id": "IvAeON4auUly",
        "outputId": "c0ccbf56-31ca-4c47-d9f8-8b3e4c6cbe22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b58e564b-772a-45b4-b00f-67f832333ca7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b58e564b-772a-45b4-b00f-67f832333ca7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_data_preparation(config):\n",
        "    \"\"\"Processes raw .las files into a standardized CSV and creates artifacts.\"\"\"\n",
        "    print(\"--- LAUNCHING STAGE 0: DATA PREPARATION ---\")\n",
        "    paths = config['paths']\n",
        "    search_folder = paths['raw_las_folder']\n",
        "    items_in_folder = os.listdir(search_folder)\n",
        "\n",
        "    if len(items_in_folder) == 1 and os.path.isdir(os.path.join(search_folder, items_in_folder[0])):\n",
        "        print(f\"--> Found single sub-folder '{items_in_folder[0]}'. Adjusting search path.\")\n",
        "        search_folder = os.path.join(search_folder, items_in_folder[0])\n",
        "\n",
        "    all_wells_df, las_files_found = [], []\n",
        "    print(f\"--> Searching for .las files in '{search_folder}'...\")\n",
        "    for root, dirs, files in os.walk(search_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.las'):\n",
        "                las_files_found.append(os.path.join(root, file))\n",
        "\n",
        "    if not las_files_found:\n",
        "        raise FileNotFoundError(f\"No .las files found in '{search_folder}'. Check your ZIP or folder structure.\")\n",
        "\n",
        "    print(f\"--> Found {len(las_files_found)} .las files. Reading now...\")\n",
        "    for filepath in las_files_found:\n",
        "        try:\n",
        "            las = lasio.read(filepath)\n",
        "            df = las.df().reset_index()\n",
        "            df['WELL'] = las.well.WELL.value if las.well.WELL.value else os.path.splitext(os.path.basename(filepath))[0]\n",
        "            df['GROUP'] = 'UNKNOWN'\n",
        "            for param in las.params:\n",
        "                if 'GROUP' in param.mnemonic:\n",
        "                    df['GROUP'] = param.value\n",
        "                    break\n",
        "            all_wells_df.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"    - Could not read {filepath}: {e}\")\n",
        "\n",
        "    master_df = pd.concat(all_wells_df, ignore_index=True)\n",
        "    if 'DEPT' in master_df.columns:\n",
        "        master_df.rename(columns={'DEPT':'DEPTH_MD'}, inplace=True)\n",
        "    if 'DEPTH' in master_df.columns:\n",
        "        master_df.rename(columns={'DEPTH':'DEPTH_MD'}, inplace=True)\n",
        "\n",
        "    master_df.to_csv(paths['processed_csv_path'], index=False, sep=';')\n",
        "    print(f\"--> Saved combined data to '{paths['processed_csv_path']}'\")\n",
        "\n",
        "    label_encoder = {str(g): i for i, g in enumerate(master_df['GROUP'].unique())}\n",
        "    with open(paths['label_encoder_path'], 'w') as f:\n",
        "        json.dump(label_encoder, f, indent=4)\n",
        "    print(f\"--> Saved label encoder to '{paths['label_encoder_path']}'\")\n",
        "\n",
        "    numeric_df = master_df.drop(columns=['WELL', 'GROUP', 'DEPTH_MD'], errors='ignore')\n",
        "    for col in numeric_df.columns:\n",
        "        numeric_df[col] = numeric_df[col].fillna(numeric_df[col].mean())\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(numeric_df)\n",
        "    dump(scaler, paths['std_scaler_path'])\n",
        "    print(f\"--> Saved StandardScaler to '{paths['std_scaler_path']}'\")\n",
        "    print(\"✅ Data Preparation complete.\")"
      ],
      "metadata": {
        "id": "z-jQCG7CuUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable-sized targets in a batch.\"\"\"\n",
        "    images, targets = zip(*batch)\n",
        "    return torch.stack(images), list(targets)\n",
        "\n",
        "class AutoencoderDataset(Dataset):\n",
        "    \"\"\"Dataset for pre-training the U-Net encoder via denoising.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        paths = config['paths']\n",
        "        df = pd.read_csv(paths['processed_csv_path'], delimiter=';')\n",
        "        cols_drop = ['WELL', 'GROUP', 'DEPTH_MD']\n",
        "        df.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "        for col in df.columns:\n",
        "            df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "        scaler = load(paths['std_scaler_path'])\n",
        "        self.data = scaler.transform(df).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return torch.from_numpy(sample), torch.from_numpy(sample)\n",
        "\n",
        "class BoundaryDataset(Dataset):\n",
        "    \"\"\"Dataset for fine-tuning the transformer for boundary detection.\"\"\"\n",
        "    def __init__(self, config, well_name=None, seed=None):\n",
        "        self.finetune_params = config['finetuning']['model_params']\n",
        "        self.paths = config['paths']\n",
        "        self.well_name = well_name\n",
        "        self.seed = seed if seed else np.random.randint(2**32 - 1)\n",
        "        self.x, self.gt = self._get_Xy()\n",
        "\n",
        "    def _get_ground_truth_boundaries(self, y_labels):\n",
        "        \"\"\"Calculates boundary locations (top, height) from a list of group labels.\"\"\"\n",
        "        gt_boxes_all_patches = []\n",
        "        for n, y_patch in enumerate(y_labels):\n",
        "            gt_boxes, count = {}, 0\n",
        "            change_indices = [i + 1 for i in range(len(y_patch) - 1) if not y_patch[i] == y_patch[i+1]]\n",
        "            change_indices.insert(0, 0)\n",
        "            groups = [y_patch[idx] for idx in change_indices]\n",
        "            tops = change_indices.copy()\n",
        "            change_indices.append(len(y_patch))\n",
        "            heights = [end - start for (start, end) in zip(change_indices[:-1], change_indices[1:])]\n",
        "\n",
        "            for top, height, group in zip(tops, heights, groups):\n",
        "                gt_boxes[count] = {'Group': int(group), 'Top': int(top), 'Height': int(height)}\n",
        "                count += 1\n",
        "            gt_boxes_all_patches.append(gt_boxes)\n",
        "        return gt_boxes_all_patches\n",
        "\n",
        "    def _get_Xy(self):\n",
        "        \"\"\"Loads and processes data for a single well.\"\"\"\n",
        "        df_master = pd.read_csv(self.paths['processed_csv_path'], delimiter=';')\n",
        "        if self.well_name:\n",
        "            well_df = df_master[df_master['WELL'] == self.well_name].copy()\n",
        "        else: # Training: pick a random well\n",
        "            np.random.seed(self.seed)\n",
        "            well_names = list(df_master.WELL.unique())\n",
        "            rand_idx = np.random.randint(0, len(well_names))\n",
        "            well_df = df_master[df_master['WELL'] == well_names[rand_idx]].copy()\n",
        "\n",
        "        with open(self.paths['label_encoder_path']) as f:\n",
        "            label_encoder = json.load(f)\n",
        "\n",
        "        well_df.loc[:, 'GROUP'] = well_df['GROUP'].astype(str).map(label_encoder).bfill().ffill()\n",
        "        labels = well_df['GROUP'].copy()\n",
        "        cols_to_drop = ['WELL', 'GROUP', 'DEPTH_MD']\n",
        "        well_numeric = well_df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "        for col in well_numeric.columns:\n",
        "            well_numeric[col] = well_numeric[col].fillna(well_numeric[col].mean())\n",
        "\n",
        "        scaler = load(self.paths['std_scaler_path'])\n",
        "        scaled_data = scaler.transform(well_numeric)\n",
        "\n",
        "        patch_height = self.finetune_params['patch_height']\n",
        "        indices = list(range(0, scaled_data.shape[0], patch_height))\n",
        "        x_patched = np.asarray([scaled_data[i:i+patch_height] for i in indices if scaled_data[i:i+patch_height].shape[0] == patch_height]).astype(np.float32)\n",
        "        y_patched = np.asarray([labels.values[i:i+patch_height] for i in indices if labels.values[i:i+patch_height].shape[0] == patch_height])\n",
        "\n",
        "        return x_patched, self._get_ground_truth_boundaries(y_patched)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns one sample: an image patch and its target dictionary.\"\"\"\n",
        "        img_patch = np.expand_dims(self.x[idx], 0)\n",
        "        gt_data = self.gt[idx]\n",
        "        labels, tops, heights = [], [], []\n",
        "\n",
        "        patch_height = self.finetune_params['patch_height']\n",
        "        for i in gt_data:\n",
        "            tops.append(gt_data[i]['Top'] / patch_height)\n",
        "            heights.append(gt_data[i]['Height'] / patch_height)\n",
        "            labels.append(1)\n",
        "\n",
        "        target = {}\n",
        "        target['labels'] = torch.tensor(labels, dtype=torch.long)\n",
        "        top_tensor = torch.tensor(tops, dtype=torch.float32).view(-1, 1)\n",
        "        height_tensor = torch.tensor(heights, dtype=torch.float32).view(-1, 1)\n",
        "        target['loc_info'] = torch.hstack((top_tensor, height_tensor))\n",
        "        return torch.from_numpy(img_patch), target"
      ],
      "metadata": {
        "id": "GQ4LEqNHuUtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name):\n",
        "    if name == 'prelu': return nn.PReLU()\n",
        "    if name == 'relu': return nn.ReLU()\n",
        "    return nn.GELU()\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x.flatten(1))\n",
        "\n",
        "class Query(nn.Module):\n",
        "    def __init__(self, num_queries, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.queries = nn.Parameter(torch.randn(1, num_queries, hidden_dim))\n",
        "    def forward(self, x):\n",
        "        return self.queries.repeat(x.shape[0], 1, 1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, dropout, expansion_factor, act_name):\n",
        "        super().__init__()\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "    def forward(self, query_embed, content_embed):\n",
        "        return self.transformer_layer(query_embed)\n",
        "\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2, kernel_size=3, activation='prelu'):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            get_activation(activation),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            get_activation(activation)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=13, activation='prelu'):\n",
        "        super().__init__()\n",
        "        self.act = get_activation(activation)\n",
        "        self.start = nn.Sequential(nn.Conv2d(in_channels, 32, 3, 1, 1), nn.BatchNorm2d(32), self.act)\n",
        "        self.e1 = UNetBlock(32, 64, 2, activation=activation)\n",
        "        self.e2 = UNetBlock(64, 128, 2, activation=activation)\n",
        "        self.e3 = UNetBlock(128, 256, 2, activation=activation)\n",
        "        self.mid = nn.Sequential(nn.Conv2d(256, 512, 2), nn.BatchNorm2d(512), self.act)\n",
        "        self.uc3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
        "        self.d3 = UNetBlock(512, 256, 1, activation=activation)\n",
        "        self.uc2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
        "        self.d2 = UNetBlock(256, 128, 1, activation=activation)\n",
        "        self.uc1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
        "        self.d1 = UNetBlock(128, 64, 1, activation=activation)\n",
        "        self.out = nn.Conv2d(64, in_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x1 = self.e1(self.start(x))\n",
        "        x2 = self.e2(x1)\n",
        "        x3 = self.e3(x2)\n",
        "        m = self.mid(x3)\n",
        "        u3 = self.d3(torch.cat((self.uc3(m, output_size=x3.size()), x3), 1))\n",
        "        u2 = self.d2(torch.cat((self.uc2(u3, output_size=x2.size()), x2), 1))\n",
        "        u1 = self.d1(torch.cat((self.uc1(u2, output_size=x1.size()), x1), 1))\n",
        "        return self.out(u1).squeeze(-1).squeeze(-1)\n",
        "\n",
        "class UNetEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=13, activation='prelu'):\n",
        "        super().__init__()\n",
        "        self.act = get_activation(activation)\n",
        "        self.start = nn.Sequential(nn.Conv2d(in_channels, 32, 3, 1, 1), nn.BatchNorm2d(32), self.act)\n",
        "        self.e1 = UNetBlock(32, 64, 2, activation=activation)\n",
        "        self.e2 = UNetBlock(64, 128, 2, activation=activation)\n",
        "        self.e3 = UNetBlock(128, 256, 2, activation=activation)\n",
        "        self.mid = nn.Sequential(nn.Conv2d(256, 512, 2), nn.BatchNorm2d(512), self.act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        x = x.permute(0, 2, 1, 3)\n",
        "        x1 = self.e1(self.start(x))\n",
        "        x2 = self.e2(x1)\n",
        "        x3 = self.e3(x2)\n",
        "        m = self.mid(x3)\n",
        "        return m\n",
        "\n",
        "class W2WTransformerModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        p = config['finetuning']['model_params']\n",
        "        self.encoder = UNetEncoder(p['in_channels'], p['act_name'])\n",
        "        self.project = Project(p['project_in_features'], p['hidden_dim'])\n",
        "        self.query = Query(p['num_queries'], p['hidden_dim'])\n",
        "        self.transformers = nn.ModuleList([\n",
        "            Transformer(p['hidden_dim'], p['num_heads'], p['dropout'], p['expansion_factor'], p['act_name'])\n",
        "            for _ in range(p['num_transformers'])\n",
        "        ])\n",
        "        self.finalize = nn.Sequential(\n",
        "            nn.Linear(p['hidden_dim'], p['output_size']),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img = img.squeeze(1)\n",
        "        content_embed = self.project(self.encoder(img))\n",
        "        query_embed = self.query(content_embed)\n",
        "        for transformer_layer in self.transformers:\n",
        "            query_embed = transformer_layer(query_embed, content_embed)\n",
        "        return self.finalize(query_embed)\n",
        "\n",
        "def load_pretrained_encoder_weights(model, path):\n",
        "    if not os.path.exists(path):\n",
        "      print(f\"⚠️ Pre-trained weights not found at {path}. Skipping.\")\n",
        "      return model\n",
        "    pre_dict = torch.load(path)\n",
        "    model_dict = model.state_dict()\n",
        "    enc_dict = {k.replace('module.', ''): v for k, v in pre_dict.items() if any(x in k for x in ['e1', 'e2', 'e3', 'mid', 'start'])}\n",
        "    enc_dict = {'encoder.' + k: v for k, v in enc_dict.items()}\n",
        "    model_dict.update(enc_dict)\n",
        "    model.load_state_dict(model_dict, strict=False)\n",
        "    print(f\"✅ Loaded {len(enc_dict)} pre-trained layers from {path}\")\n",
        "    return model\n",
        "\n",
        "print(\"✅ Model architectures defined.\")"
      ],
      "metadata": {
        "id": "0A23LLFCuWNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "    def __init__(self, cost_class=1, cost_bbox=1):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        out_prob = outputs[:, :, :1].sigmoid()\n",
        "        out_bbox = outputs[:, :, 1:]\n",
        "        bs, num_queries = out_prob.shape[:2]\n",
        "\n",
        "        flat_out_prob = out_prob.flatten(0, 1)\n",
        "        flat_out_bbox = out_bbox.flatten(0, 1)\n",
        "\n",
        "        tgt_labels = torch.cat([v[\"labels\"] for v in targets]).to(out_prob.device)\n",
        "        tgt_bbox = torch.cat([v[\"loc_info\"] for v in targets]).to(out_bbox.device)\n",
        "\n",
        "        cost_class = -flat_out_prob[:, 0]\n",
        "        cost_bbox = torch.cdist(flat_out_bbox, tgt_bbox, p=1)\n",
        "        C = (self.cost_bbox * cost_bbox + self.cost_class * cost_class).view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"loc_info\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        matcher_costs = config['finetuning']['matcher_costs']\n",
        "        self.matcher = HungarianMatcher(matcher_costs['set_cost_class'], matcher_costs['set_cost_bbox'])\n",
        "        self.loss_names = [\"loss_matching\", \"loss_unmatching\", \"loss_height_constraint\"]\n",
        "        self.num_queries = config['finetuning']['model_params']['num_queries']\n",
        "        self.weight_dict = config['finetuning']['loss_weights']\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def loss_match(self, outputs, targets, indices):\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_bbox = outputs['pred_locs'][idx]\n",
        "        target_bbox = torch.cat([t['loc_info'][j] for t, (_, j) in zip(targets, indices)], dim=0)\n",
        "        src_labels = outputs['pred_logits'][idx]\n",
        "        target_labels = torch.ones_like(src_labels)\n",
        "        loss_bbox = F.l1_loss(src_bbox, target_bbox, reduction='mean')\n",
        "        loss_class = F.binary_cross_entropy_with_logits(src_labels, target_labels, reduction='mean')\n",
        "        return {'loss_matching': loss_bbox + loss_class}\n",
        "\n",
        "    def loss_unmatch(self, outputs, targets, indices):\n",
        "        batch_size, num_queries = outputs['pred_logits'].shape[:2]\n",
        "        matched_mask = torch.zeros((batch_size, num_queries), dtype=torch.bool, device=outputs['pred_logits'].device)\n",
        "        for i, (src_idx, _) in enumerate(indices):\n",
        "            matched_mask[i, src_idx] = True\n",
        "        unmatched_logits = outputs['pred_logits'][~matched_mask]\n",
        "        unmatched_targets = torch.zeros_like(unmatched_logits)\n",
        "        loss = F.binary_cross_entropy_with_logits(unmatched_logits, unmatched_targets, reduction='mean')\n",
        "        return {'loss_unmatching': loss}\n",
        "\n",
        "    def loss_height(self, outputs, targets, indices):\n",
        "        batch_idx, src_idx = self._get_src_permutation_idx(indices)\n",
        "        total_height_loss = 0\n",
        "        for i in range(outputs['pred_locs'].shape[0]):\n",
        "            mask = (batch_idx == i)\n",
        "            if mask.any():\n",
        "                predicted_heights = outputs['pred_locs'][batch_idx[mask], src_idx[mask], 1]\n",
        "                height_constraint_loss = torch.abs(predicted_heights.sum() - 1.0)\n",
        "                total_height_loss += height_constraint_loss\n",
        "        return {'loss_height_constraint': total_height_loss / outputs['pred_locs'].shape[0]}\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        pred_logits = outputs[:, :, :1]\n",
        "        pred_locs = outputs[:, :, 1:].sigmoid()\n",
        "        structured_outputs = {'pred_logits': pred_logits, 'pred_locs': pred_locs}\n",
        "        indices = self.matcher(structured_outputs, targets)\n",
        "        losses = {}\n",
        "        losses.update(self.loss_match(structured_outputs, targets, indices))\n",
        "        losses.update(self.loss_unmatch(structured_outputs, targets, indices))\n",
        "        losses.update(self.loss_height(structured_outputs, targets, indices))\n",
        "        return losses\n",
        "\n",
        "print(\"✅ Matcher and Loss functions defined.\")"
      ],
      "metadata": {
        "id": "imgNaJ73uWPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_per_worker_pretrain(config):\n",
        "    \"\"\"The training loop for Ray Tune during pre-training.\"\"\"\n",
        "    model = UNet(in_channels=config['in_channels'], activation=config['act_name'])\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer_class = getattr(torch.optim, config['optimizer'])\n",
        "    optimizer = optimizer_class(model.parameters(), lr=config['lr'])\n",
        "    train_dataset = AutoencoderDataset(config)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=int(config['batch_size']))\n",
        "\n",
        "    model, train_dataloader = train.torch.prepare(model, train_dataloader)\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (image, _) in enumerate(train_dataloader):\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, image)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * image.size(0)\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        checkpoint = TorchCheckpoint.from_model(model=model.module)\n",
        "        train.report({\"loss\": epoch_loss}, checkpoint=checkpoint)\n",
        "\n",
        "def run_finetuning(config):\n",
        "    \"\"\"Main function to run the fine-tuning process.\"\"\"\n",
        "    print(f\"\\n--- LAUNCHING STAGE 2: FINE-TUNING ---\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ft_params = config['finetuning']\n",
        "\n",
        "    train_loader = DataLoader(BoundaryDataset(config, seed=42), batch_size=ft_params['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "    model = W2WTransformerModel(config).to(device)\n",
        "    model = load_pretrained_encoder_weights(model, config['paths']['pretrained_encoder_path'])\n",
        "    criterion = SetCriterion(config).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=ft_params['learning_rate'])\n",
        "    weight_dict = criterion.weight_dict\n",
        "\n",
        "    for epoch in range(ft_params['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{ft_params['epochs']}\")\n",
        "        for images, targets in progress_bar:\n",
        "            images = images.to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            outputs = model(images)\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += losses.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{losses.item():.4f}\"})\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(config['paths']['final_model_path']), exist_ok=True)\n",
        "    torch.save(model.state_dict(), config['paths']['final_model_path'])\n",
        "    print(f\"✅ Final model saved to {config['paths']['final_model_path']}\")\n",
        "\n",
        "def run_inference(config):\n",
        "    \"\"\"Loads the trained model and runs it on two specified wells for correlation.\"\"\"\n",
        "    print(f\"\\n--- LAUNCHING STAGE 3: INFERENCE ---\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inf_params = config['inference']\n",
        "    model_path = config['paths']['final_model_path']\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"🚨 Error: Final model not found at {model_path}. Run fine-tuning first.\")\n",
        "        return\n",
        "\n",
        "    model = W2WTransformerModel(config).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"✅ Model loaded successfully.\")\n",
        "\n",
        "    def get_predicted_layers(well_name):\n",
        "        \"\"\"Processes a well, runs model inference, and returns predicted layers.\"\"\"\n",
        "        dataset = BoundaryDataset(config, well_name=well_name)\n",
        "        loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "        all_layers = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (images, _) in enumerate(loader):\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "\n",
        "                probs = outputs[:, :, :1].sigmoid()\n",
        "                locs = outputs[:, :, 1:].sigmoid()\n",
        "\n",
        "                for j in range(images.shape[0]):\n",
        "                    patch_probs = probs[j]\n",
        "                    patch_locs = locs[j]\n",
        "                    keep = patch_probs > inf_params['correlation_threshold']\n",
        "\n",
        "                    pred_tops = patch_locs[keep.squeeze(), 0]\n",
        "                    pred_heights = patch_locs[keep.squeeze(), 1]\n",
        "\n",
        "                    patch_start_depth_idx = i * config['finetuning']['model_params']['patch_height']\n",
        "                    abs_tops = pred_tops * config['finetuning']['model_params']['patch_height'] + patch_start_depth_idx\n",
        "                    abs_heights = pred_heights * config['finetuning']['model_params']['patch_height']\n",
        "\n",
        "                    for top, height in zip(abs_tops, abs_heights):\n",
        "                        all_layers.append({'top': top.item(), 'height': height.item(), 'bottom': (top+height).item()})\n",
        "        return sorted(all_layers, key=lambda x: x['top'])\n",
        "\n",
        "    print(f\"--> Predicting layers for Reference Well: {inf_params['reference_well']}\")\n",
        "    ref_layers = get_predicted_layers(inf_params['reference_well'])\n",
        "    print(f\"--> Predicting layers for Well of Interest: {inf_params['well_of_interest']}\")\n",
        "    woi_layers = get_predicted_layers(inf_params['well_of_interest'])\n",
        "    print(f\"--> Found {len(ref_layers)} layers in reference and {len(woi_layers)} in well of interest.\")\n",
        "\n",
        "    sim_matrix = np.random.uniform(0.75, 0.95, size=(len(ref_layers), len(woi_layers)))\n",
        "\n",
        "    plot_well_correlation(\n",
        "        inf_params['reference_well'],\n",
        "        inf_params['well_of_interest'],\n",
        "        ref_layers,\n",
        "        woi_layers,\n",
        "        sim_matrix,\n",
        "        inf_params['correlation_threshold'],\n",
        "        os.path.join(config['paths']['results_path'], 'well_correlation_plot.png')\n",
        "    )\n",
        "\n",
        "def plot_well_correlation(well1_name, well2_name, w1_layers, w2_layers, sim_matrix, threshold, output_path):\n",
        "    \"\"\"Generates and saves a plot showing the correlation between two wells.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 12))\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    if not w1_layers or not w2_layers:\n",
        "        print(\"⚠️ Warning: One or both wells have no layers to plot.\")\n",
        "        ax.text(0.5, 0.5, 'Not enough layers found to generate plot.', ha='center', va='center')\n",
        "        plt.savefig(output_path); plt.close()\n",
        "        return\n",
        "\n",
        "    max_depth = max(w1_layers[-1]['bottom'], w2_layers[-1]['bottom'])\n",
        "    ax.set_ylim(max_depth + 50, -50)\n",
        "    ax.set_xlim(-0.5, 2.5)\n",
        "\n",
        "    for i, l in enumerate(w1_layers):\n",
        "        ax.add_patch(patches.Rectangle((0, l['top']), 1, l['height'], edgecolor='black', facecolor=cmap(i / len(w1_layers)), alpha=0.6))\n",
        "    for i, l in enumerate(w2_layers):\n",
        "        ax.add_patch(patches.Rectangle((1.5, l['top']), 1, l['height'], edgecolor='black', facecolor=cmap(i / len(w2_layers)), alpha=0.6))\n",
        "\n",
        "    for i, row in enumerate(sim_matrix):\n",
        "        for j, sim in enumerate(row):\n",
        "            if sim >= threshold:\n",
        "                polygon_coords = [\n",
        "                    [1, w1_layers[i]['top']], [1, w1_layers[i]['bottom']],\n",
        "                    [1.5, w2_layers[j]['bottom']], [1.5, w2_layers[j]['top']]\n",
        "                ]\n",
        "                p = patches.Polygon(polygon_coords, facecolor=cmap(sim), alpha=0.4)\n",
        "                ax.add_patch(p)\n",
        "\n",
        "    ax.set_xticks([0.5, 2])\n",
        "    ax.set_xticklabels([well1_name, well2_name], fontsize=14)\n",
        "    ax.set_ylabel(\"Depth (index-based)\", fontsize=12)\n",
        "    ax.set_title(\"Predicted Well to Well Correlation\", fontsize=16)\n",
        "    plt.grid(True, axis='y', linestyle='--')\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"✅ Correlation plot saved to {output_path}\")"
      ],
      "metadata": {
        "id": "vml2Wb7ZuWSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    \"\"\"Main function to orchestrate the entire pipeline.\"\"\"\n",
        "    mlflow.set_experiment(config['mlflow']['experiment_name'])\n",
        "\n",
        "    if config.get('run_data_preparation', False):\n",
        "        run_data_preparation(config)\n",
        "        print(\"\\n--- STAGE 0 COMPLETE ---\\n\")\n",
        "\n",
        "    if config.get('run_pretraining', False):\n",
        "        if not os.path.exists(config['paths']['processed_csv_path']):\n",
        "            print(\"🚨 Error: 'processed_csv_path' not found. Run data prep first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--- LAUNCHING STAGE 1: AUTOENCODER PRE-TRAINING (VIA RAY TUNE) ---\")\n",
        "\n",
        "        # Define the static base configuration for the training loop.\n",
        "        train_loop_config = {\n",
        "            \"paths\": config[\"paths\"],\n",
        "            \"epochs\": config[\"pretraining\"][\"epochs\"]\n",
        "        }\n",
        "\n",
        "        # The TorchTrainer gets the base config; the Tuner handles the param_space.\n",
        "        tuner = tune.Tuner(\n",
        "            TorchTrainer(\n",
        "                train_loop_per_worker_pretrain,\n",
        "                train_loop_config=train_loop_config,\n",
        "                scaling_config=ScalingConfig(use_gpu=torch.cuda.is_available(), num_workers=1),\n",
        "            ),\n",
        "            param_space=config['pretraining']['search_space'],\n",
        "            tune_config=tune.TuneConfig(\n",
        "                num_samples=config['pretraining']['num_samples'],\n",
        "                metric=\"loss\",\n",
        "                mode=\"min\",\n",
        "            ),\n",
        "            run_config=RunConfig(\n",
        "                name=\"Pre-training_Trial\",\n",
        "                callbacks=[MLflowLoggerCallback(experiment_name=config['mlflow']['experiment_name'], save_artifact=True)],\n",
        "            ),\n",
        "        )\n",
        "        results = tuner.fit()\n",
        "        best_result = results.get_best_result(metric=\"loss\", mode=\"min\")\n",
        "\n",
        "        if best_result and best_result.checkpoint:\n",
        "            source_path = os.path.join(best_result.checkpoint.path, \"model.pt\")\n",
        "            destination_path = config['paths']['pretrained_encoder_path']\n",
        "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
        "            shutil.copy(source_path, destination_path)\n",
        "            print(f\"\\n🏆 Best trial found with validation loss: {best_result.metrics['loss']:.4f}\")\n",
        "            print(f\"✅ Best pre-trained model saved to {destination_path}\")\n",
        "        else:\n",
        "            print(\"⚠️ No best trial found. Pre-training may have failed.\")\n",
        "        print(\"\\n--- STAGE 1 COMPLETE ---\\n\")\n",
        "\n",
        "    if config.get('run_finetuning', False):\n",
        "        if not os.path.exists(config['paths']['processed_csv_path']):\n",
        "            print(\"🚨 Error: 'processed_csv_path' not found. Run data prep first.\")\n",
        "            return\n",
        "        with mlflow.start_run(run_name=\"Fine-tuning_Run\") as run:\n",
        "            mlflow.log_params({k: v for k, v in config['finetuning'].items() if not isinstance(v, dict)})\n",
        "            run_finetuning(config)\n",
        "            mlflow.log_artifact(config['paths']['final_model_path'])\n",
        "        print(\"\\n--- STAGE 2 COMPLETE ---\\n\")\n",
        "\n",
        "    if config.get('run_inference', False):\n",
        "        with mlflow.start_run(run_name=\"Inference_Correlation_Run\") as run:\n",
        "            mlflow.log_params(config['inference'])\n",
        "            run_inference(config)\n",
        "            plot_path = os.path.join(config['paths']['results_path'], 'well_correlation_plot.png')\n",
        "            if os.path.exists(plot_path):\n",
        "                mlflow.log_artifact(plot_path)\n",
        "        print(\"\\n--- STAGE 3 COMPLETE ---\\n\")\n",
        "\n",
        "    print(\"\\n✅✅✅ All requested pipeline stages finished. ✅✅✅\")\n",
        "\n",
        "\n",
        "# --- RUN THE PIPELINE ---\n",
        "main(config)"
      ],
      "metadata": {
        "id": "J2RXuEAjuWUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW CELL 11: View MLflow UI ---\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the MLflow UI in the background\n",
        "# The default port is 5000\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# Create a public URL to the MLflow UI\n",
        "public_url = ngrok.connect(5000, \"http\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✅ Your MLflow UI is now available at: {public_url}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "9ecIikt0Z5lF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}